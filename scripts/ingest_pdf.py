# ingest_pdf_qdrant.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Script de carga e indexaciÃ³n de documentos PDF
# Proyecto: Knowledge Navigator
# FunciÃ³n: Procesar PDFs â†’ dividir en fragmentos â†’ generar embeddings â†’ indexar en Qdrant Cloud
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_qdrant import QdrantVectorStore
from langchain_ollama import OllamaEmbeddings
# 0. ğŸ” Cargar variables de entorno desde .env
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = "knowledge-navigator"
PDF_DIR = "data/pdfs"

# 1. ğŸ“¥ Cargar documentos PDF con metadatos por pÃ¡gina
def cargar_documentos(pdf_dir):
    documentos = []
    paginas_ignoradas = 0

    print("ğŸ“¥ Cargando documentos...")

    for filename in os.listdir(pdf_dir):
        if filename.endswith(".pdf"):
            path = os.path.join(pdf_dir, filename)
            loader = PyPDFLoader(path)
            docs_por_pagina = loader.load()
            
            for i, doc in enumerate(docs_por_pagina):
                # Ignorar pÃ¡ginas vacÃ­as
                if not doc.page_content.strip():
                    print(f"âš ï¸ PÃ¡gina vacÃ­a ignorada: {filename}, pÃ¡gina {i+1}")
                    paginas_ignoradas += 1
                    continue

                doc.metadata["source"] = filename
                doc.metadata["page"] = i + 1
                documentos.append(doc)
    
    print(f"âœ… Se han cargado {len(documentos)} pÃ¡ginas vÃ¡lidas de documentos.")
    return documentos

# 2. âœ‚ï¸ Fragmentar documentos en chunks con solapamiento
def dividir_en_fragmentos(documentos):
    print("âœ‚ï¸ Dividiendo en fragmentos...")
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", " "]
    )
    todos_los_chunks = splitter.split_documents(documentos)
    chunks_utiles = [c for c in todos_los_chunks if es_fragmento_util(c.page_content)]
    print(f"âœ… Fragmentos generados: {len(chunks_utiles)} (de {len(todos_los_chunks)} totales)")
    return chunks_utiles

# ğŸ” Filtro para eliminar fragmentos irrelevantes o genÃ©ricos
def es_fragmento_util(texto: str) -> bool:
    texto = texto.strip().lower()
    return (
        len(texto) >= 300 and
        not texto.startswith("Ã­ndice") and
        "copyright" not in texto and
        not texto.isdigit()
    )

# 3. ğŸ“¡ Conectar con Qdrant y almacenar los fragmentos
def indexar_en_qdrant(chunks):
    embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")
    print("ğŸ“¡ Conectando con Qdrant Cloud...")
    print(f"ğŸ“¦ Guardando en colecciÃ³n '{COLLECTION_NAME}'...")
    vectorstore = QdrantVectorStore.from_documents(
    documents=chunks,
    embedding=embeddings,
    url= QDRANT_URL,
    api_key=QDRANT_API_KEY,
    collection_name=COLLECTION_NAME,
    # prefer_grpc=False,
    force_recreate=True  # ğŸŸ¢ AÃ±ade esto para evitar conflicto dimensional
)

    print("âœ… Â¡Vector store cargado exitosamente en Qdrant!")

# ğŸ EjecuciÃ³n principal
if __name__ == "__main__":
    documentos = cargar_documentos(PDF_DIR)
    chunks = dividir_en_fragmentos(documentos)
    # genembeddings = generar_embeddings()
    indexar_en_qdrant(chunks)
    print(f"ğŸ¯ ColecciÃ³n '{COLLECTION_NAME}' creada y cargada con Ã©xito en Qdrant Cloud.")
