# =============================================================
# razonador_cot.py â€” VersiÃ³n definitiva con historial conversacional y respuesta pedagÃ³gica o concisa
# =============================================================

from langchain.chat_models import init_chat_model
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# =============================================================
# FUNCIÃ“N PRINCIPAL: RESPUESTA A PARTIR DE CONTEXTO (local)
# =============================================================
def generar_respuesta(pregunta: str, contexto: str = "", historial: str = "", temperatura: float = 0.2, modo_conciso: bool = False) -> str:
    # SYSTEM PROMPT
    system_prompt = (
        "Eres un asistente conversacional que responde de forma breve, clara y precisa, sin explicaciones." if modo_conciso else
        "Eres un asistente experto que responde con precisiÃ³n y pedagogÃ­a. Usa el siguiente formato:\n\n"
        "DefiniciÃ³n textual o normativa: <respuesta principal>\n"
        "ExplicaciÃ³n ampliada: <explicaciÃ³n complementaria y contextual>\n\n"
        "Siempre responde en espaÃ±ol, con tono profesional y accesible."
    )

    # HUMAN PROMPT
    if historial.strip():
        human_prompt = (
            "Esta es una conversaciÃ³n continua.\n\n"
            "Historial previo:\n{historial}\n\n"
            "Contexto relevante:\n{contexto}\n\n"
            "Pregunta actual:\n{pregunta}"
        )
        input_data = {"pregunta": pregunta, "contexto": contexto, "historial": historial}
    elif contexto.strip():
        human_prompt = "Contexto relevante:\n{contexto}\n\nPregunta: {pregunta}"
        input_data = {"pregunta": pregunta, "contexto": contexto}
    else:
        human_prompt = "Pregunta: {pregunta}"
        input_data = {"pregunta": pregunta}

    try:
        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_prompt),
            HumanMessagePromptTemplate.from_template(human_prompt)
        ])
        modelo = init_chat_model("gemini-2.5-flash", model_provider="google_genai", temperature=temperatura)
        respuesta = (prompt | modelo).invoke(input_data)
        return respuesta.content if hasattr(respuesta, "content") else str(respuesta)
    except Exception as e:
        return f"âŒ Error al generar respuesta: {e}"

# =============================================================
# FUNCIÃ“N SECUNDARIA: RESPUESTA A PARTIR DE FRAGMENTOS WEB
# =============================================================
def generar_respuesta_web(pregunta: str, contexto: str) -> str:
    prompt_web = ChatPromptTemplate.from_template(
        """
        Has recibido resultados de una bÃºsqueda web sobre la siguiente pregunta:

        ğŸ§  Pregunta del usuario:
        {pregunta}

        ğŸŒ Fragmentos encontrados:
        {contexto}

        Redacta una respuesta clara y precisa basada SOLO en los fragmentos anteriores.
        No inventes informaciÃ³n. Usa este formato:

        ğŸ“˜ Respuesta principal
        <respuesta>

        ğŸ’¡ InformaciÃ³n adicional
        <si hay algo Ãºtil que aÃ±adir, escrÃ­belo; si no, indica que no hay informaciÃ³n adicional>
        """
    )
    try:
        modelo = init_chat_model("gemini-2.5-flash", model_provider="google_genai", temperature=0)
        respuesta = (prompt_web | modelo).invoke({"pregunta": pregunta, "contexto": contexto})
        return respuesta.content if hasattr(respuesta, "content") else str(respuesta)
    except Exception as e:
        return f"âŒ Error al generar respuesta web: {e}"
