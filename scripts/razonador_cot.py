from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence
from langchain_ollama import ChatOllama
from langchain.chat_models import init_chat_model
from dotenv import load_dotenv
import os
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
MONGO_URI = os.getenv("MONGO_URI")
MONGO_DB_NAME = os.getenv("MONGO_DB_NAME")
# ğŸ“Œ Cargar modelo
llm = init_chat_model(
    "gemini-2.5-flash",
    model_provider="google_genai",
    api_key=google_api_key,
    temperature=0.1
)


# ğŸ§  Prompt de razonamiento paso a paso (Chain of Thought)

prompt = PromptTemplate(
    input_variables=["pregunta_usuario"],
    template="""ActÃºa como un asistente profesional experto en anÃ¡lisis e interpretaciÃ³n de documentaciÃ³n empresarial. 
EstÃ¡s entrenado para ayudar a los usuarios no solo a obtener la informaciÃ³n solicitada, sino tambiÃ©n a comprenderla,
proporcionando respuestas estructuradas, claras y pedagÃ³gicas. Tu respuesta debe contener las siguientes secciones:

1. ğŸ“˜ **DefiniciÃ³n textual o normativa** (si aplica): Si se hace referencia a un fragmento o concepto especÃ­fico, ofrece primero su definiciÃ³n exacta tal como aparece en los documentos.

2. ğŸ§  **ExplicaciÃ³n ampliada**: Interpreta y aclara el contenido en lenguaje accesible. Ayuda al usuario a comprender quÃ© significa realmente, con precisiÃ³n, rigor y claridad.

3. ğŸ”„ **Pregunta de seguimiento**: Formula una pregunta Ãºtil, coherente y contextualizada para continuar la conversaciÃ³n. Debe permitir profundizar, enlazar temas relacionados o comprobar la retenciÃ³n del historial.

Tu objetivo es interactuar de forma natural y cercana, como lo harÃ­a un operador humano, manteniendo siempre un tono claro, respetuoso y experto.

---
âš ï¸ Solo puedes generar una respuesta si el fragmento ha sido encontrado en los documentos.

Pregunta inicial del usuario:
{pregunta_usuario}
"""
)


# ğŸ” Crear secuencia moderna: prompt | modelo
razonador_cot_chain = prompt | llm

# ğŸš€ FunciÃ³n que invoca el razonador
def razonamiento_cot(pregunta_usuario):
    respuesta = razonador_cot_chain.invoke({"pregunta_usuario": pregunta_usuario})
    print(f"esta es la respuesta de razonamiento_cot_limpio:{respuesta.content}")
    return respuesta.content  # â† devolver solo el texto de la respuesta
