# ======================================
# razonador_cot.py (con soporte web y formato estructurado)
# ======================================
from langchain.chat_models import init_chat_model
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate

# Funci贸n para respuestas normativas o t茅cnicas

def generar_respuesta(pregunta: str, contexto: str = "", temperatura: float = 0.2, modo_conciso: bool = False) -> str:
    if modo_conciso:
        system_prompt = (
            "Eres un asistente de IA que responde de forma breve, precisa y objetiva. "
            "Responde **solo en espa帽ol** y proporciona una sola frase clara con el dato exacto que responde a la pregunta, "
            "sin explicaciones adicionales ni justificaciones. No uses encabezados ni subt铆tulos. "
            "Ejemplo de formato: 'Hoy es viernes 25 de julio de 2025.'"
        )
    else:
        system_prompt = (
            "Eres un asistente de IA experto en definiciones normativas y conceptos t茅cnicos. "
            "Responde **solo en espa帽ol** y sigue el formato indicado a continuaci贸n para tu respuesta:\n\n"
            "Definici贸n textual o normativa: <una definici贸n concisa, citando normativa textual si es relevante>\n"
            "Explicaci贸n ampliada: <una explicaci贸n detallada y pedag贸gica del concepto, aportando contexto adicional>\n\n"
            "El tono debe ser profesional, claro y pedag贸gico."
        )

    if contexto and contexto.strip():
        human_prompt = "Contexto relevante:\n{contexto}\n\nPregunta: {pregunta}"
        input_data = {"pregunta": pregunta, "contexto": contexto}
    else:
        human_prompt = "Pregunta: {pregunta}"
        input_data = {"pregunta": pregunta}

    try:
        prompt_template = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_prompt),
            HumanMessagePromptTemplate.from_template(human_prompt)
        ])
        modelo = init_chat_model("gemini-2.5-flash", model_provider="google_genai", temperature=temperatura)
        cadena = prompt_template | modelo
        respuesta_modelo = cadena.invoke(input_data)
        texto_respuesta = respuesta_modelo if isinstance(respuesta_modelo, str) else str(respuesta_modelo)
        return texto_respuesta
    except Exception as e:
        return f"Error al generar respuesta con el modelo: {e}"


# Funci贸n espec铆fica para respuestas a partir de snippets web

def generar_respuesta_web(pregunta: str, contexto: str) -> str:
    prompt_web = ChatPromptTemplate.from_template("""
A continuaci贸n se presentan fragmentos de resultados de b煤squeda web relacionados con la siguiente pregunta:

Pregunta: {pregunta}

Fragmentos web:
{contexto}

Tu tarea es generar una respuesta clara, objetiva, precisa y actual basada exclusivamente en estos fragmentos. 
No inventes, no generalices, no expliques c贸mo funciona un modelo de lenguaje. 
Lim铆tate a resumir o sintetizar lo que indican los fragmentos y proporciona informaci贸n verificable. Si los fragmentos son vagos, dilo.

Incluye:
 Respuesta principal
 Informaci贸n adicional (si procede)
""")

    try:
        modelo = init_chat_model("gemini-2.5-flash", model_provider="google_genai", temperature=0)
        cadena = prompt_web | modelo
        resultado = cadena.invoke({"pregunta": pregunta, "contexto": contexto})
        return resultado.content
    except Exception as e:
        return f"Error al generar respuesta web: {e}"