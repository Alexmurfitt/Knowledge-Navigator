import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_qdrant import QdrantVectorStore 
from qdrant_client import QdrantClient
from dotenv import load_dotenv
from qdrant_client import QdrantClient, models  #PAra poder acceder a la base de datos y eliminar points
import os
import tempfile
from Accion_Qdrant import *

load_dotenv()

# google_api_key = os.getenv("GOOGLE-API-KEY")    #Seleecionamos la apikey del modelo
qdrant_url = os.getenv("QDRANT-URL")
qdrant_api_key = os.getenv("QDRANT-API-KEY")
collection_name = os.getenv("COLLECTION-NAME")

crear_indice(collection_name=collection_name)

def extract_text_from_pdf(pdf_file):
    """Extrae el texto de un archivo PDF subido."""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(pdf_file.getvalue())
            tmp_file_path = tmp_file.name

        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        os.remove(tmp_file_path)
        return documents
    except Exception as e:
        st.error(f"Error al leer el PDF: {e}")
        return []
# Aqui es donde separamos el texto, para mas precision deberia bajar los chunck un poco
def split_text_into_chunks(documents):
    """Divide los documentos extra칤dos en trozos (chunks) m치s peque침os."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,
        chunk_overlap=50,
        length_function=len
    )
    chunks = text_splitter.split_documents(documents)
    return chunks

# --- Interfaz de Usuario de Streamlit ---

st.set_page_config(page_title="Cargar Documentos", page_icon="游늯")
st.title("Cargar PDFs a la Base de Conocimiento ")

st.markdown("""
Sube uno o varios archivos PDF. El contenido se procesar치 y almacenar치 en Qdrant. Si la colecci칩n no existe, se crear치 autom치ticamente.
""")

uploaded_files = st.file_uploader(
    "Elige tus archivos PDF",
    type="pdf",
    accept_multiple_files=True
)

if uploaded_files:
    if st.button("Procesar y Cargar a Qdrant"):
        with st.spinner("Procesando archivos... Este proceso puede tardar un momento."):
            
            # 1. Extraer y dividir el texto en chunks
            all_docs = []
            for pdf_file in uploaded_files:
                st.write(f"Leyendo `{pdf_file.name}`...")
                docs = extract_text_from_pdf(pdf_file)
                for doc in docs:
                    doc.metadata["document_name_id"] = pdf_file.name
                all_docs.extend(docs)
            
            chunks = split_text_into_chunks(all_docs)

            # 2. Usar QdrantVectorStore.from_documents para crear la colecci칩n y subir los datos

            try:
                st.write("Conectando con Qdrant y creando vectores...")
                
                # Inicializamos el modelo de embeddings
                embeddings = OllamaEmbeddings(model="mxbai-embed-large:latest")

                # Usamos .from_documents para crear la colecci칩n y a침adir los documentos en un solo paso
                QdrantVectorStore.from_documents(
                    documents=chunks,
                    embedding=embeddings,
                    url=qdrant_url,
                    api_key=qdrant_api_key,
                    collection_name=collection_name, # Tiene que ser le mismo nombre que tengo en Qdrant
                    force_recreate=False # Poner en False para no borrarla cada vez
                )
            

                st.success(f"춰칄xito! Se han procesado y a침adido {len(chunks)} fragmentos de texto a la colecci칩n 'Prueba_RAG'.")

            except Exception as e:
                st.error(f"Ha ocurrido un error al conectar o subir a Qdrant: {e}")

            

    


    